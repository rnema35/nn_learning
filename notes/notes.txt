how to make a neural net for the mnist data set
-> convert 28x28 images by flattening them into a 784 array
-> hopefully get to two hidden layers, but first will have only 1 w/ 15 nodes
-> 10 output nodes

-> need to take inputs
-> define layers
    -> weights and biases per node per layer
        - initialize randomly?
    -> activation function for each node

-> feedforward
    -> take nodes and pass it on

->backprop
    -> define loss funciton
    -> calculate loss
    -> determine the delta based on derivatives
    -> adjust weights and biases

-> repeat for epochs
----------------------------------------------------------------------------------

processing one point at a time:

-> repeat for # epochs
    Loss = 0
    For each point
    -> get the point
    -> get hypothesis from feedforward, loss from cost, add to loss
    -> then use back propagation to get change of weights/bias
    -> update weights
     


---------------------------------------------------------------------------------

initialiizing epochs gives me a loss of 500, but at 15 only 250, 
    what could be the case
    loss is calculated per epcoh seperaltey, the amount of epochs should not matter
-> turns out I wasn't doing my accuracy or loss function properly!

---------------------------------------------------------------------------------

RNNS

Lets do feed forward first:
-How do we get the first activation:
    Should have an initial a0 to get things started (or it could be 0)
    -> a_1 = activation_function( W_ax * X_1 + W_aa * a_0 + b_a)
    -> y_1 = activation_functoin( W_ay * a1 + b_y ) 
    So you have 5 (6?) things to initialize:
        - W_ax
        - W_aa
        - W_ay
        - b_a
        - b_y
        - a_0 but this can be kept as 0 matrix
    -> Then you feed the a_1 to layer a_2 and so and so on


what would the data to train on be?
-> make some time series data like forcasting a price
lets say i have 1000 time points:
    - I can split it into 1000 groups (990?) where i do 0-10, 1-11
        so for 0-10, i predict timepoint 11
        and for 1-11 i predict timepoint 12
    to have 1000 data points, i will have an array of 1000 x 10
        [ [10 time points]_0, []_1, .... []_999 ]


ok so what is happening
-> have a bunch of sequential data
    -> shuffle it around'
--> per batch, take a sequence, 
    -> go through it (get the the loss and add it up)
--> 